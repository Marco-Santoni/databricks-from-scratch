{"cells":[{"cell_type":"markdown","source":["## Structured Streaming\n\n[ Ref _Learning Spark v2_ book, _Chapter 3_.]\n\nTraditionally, distributed stream processing has been implemented with a record-at-a-time processing model.\n\n<img src=\"https://github.com/Marco-Santoni/databricks-from-scratch/blob/main/training-spark/img/chapter_8_traditional.png?raw=true\" width=\"500\">\n\n[Ref](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch08.html)\n\nThis processing model can achieve very low latencies—that is, an input record can be processed by the pipeline and the resulting output can be generated within milliseconds. However, this model is not very efficient at recovering from node failures and straggler nodes (i.e., nodes that are slower than others).\n\n## Micro-batch stream processing\n\nSpark Streaming introduced the idea of micro-batch stream processing, where the streaming computation is modeled as a continuous series of small, map/reduce-style batch processing jobs (hence, “micro-batches”) on small chunks of the stream data.\n\n<img src=\"https://github.com/Marco-Santoni/databricks-from-scratch/blob/main/training-spark/img/chapter_8_microbatch.png?raw=true\" width=\"500\">\n\n[ Ref _Learning Spark v2_ book, _page 208_]\n\nAs shown here, Spark Streaming divides the data from the input stream into, say, 1- second micro-batches. Each batch is processed in the Spark cluster in a distributed manner with small deterministic tasks that generate the output in micro-batches.\n\nMain advantages\n\n1. **fault-tolerance.** Recovering from failures is quick and easy thanks to Spark's task scheduling\n2. **determinism**. The deterministic nature of the tasks ensures that the output data is the same no matter how many times the task is reexecuted. This crucial characteristic enables Spark Streaming to provide end-to-end exactly-once processing guarantees, that is, the generated output results will be such that every input record was processed exactly once.\n\nMain disadvantage: **latency**. Few seconds rather than few milliseconds."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62ad45e6-3dd6-4945-803f-a7a254ee35b7"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ef87247-9b51-4a78-8cf1-750ee4aada7a"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"6_streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":431371217089634}},"nbformat":4,"nbformat_minor":0}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "917a7930-8e7b-4624-9b29-88359dfd0f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's create a first notebook and attach it to a cluster. We can choose a `Python` module and make us of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f681e172-ae95-4457-b663-ca9fe3f31d7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a852dd38-b31a-4038-8d44-d89ca3240cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where are we actually sitting? What files are available to us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5bb1cfa1-5717-484c-a25c-a83acbd81a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "535b6f5f-868d-47ec-b0cb-daa33055b73d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81ecd4bc-f90e-48cf-8c44-fa8928b0ca8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls databricks-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33baa889-f3df-4db9-b30a-8f241424c6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "[DBFS](https://docs.databricks.com/data/databricks-file-system.html) is an abstraction on top of scalable object storage and offers the following benefits:\n",
    "\n",
    "- Allows you to mount storage objects so that you can seamlessly access data without requiring credentials.\n",
    "- Allows you to interact with object storage using directory and file semantics instead of storage URLs.\n",
    "- Persists files to object storage, so you won’t lose data after you terminate a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57cb8734-2e47-4439-a165-403cf29b2a99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where can I play around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe7dcdd9-403d-4605-a647-c05e7000b055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls FileStore/shared_uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c818ff6-ef91-4bca-86e1-c703dabdc345",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read\n",
    "\n",
    "We'll now read some files as strings and as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b73a4a8d-bfd4-4e25-a4f5-c13c341000f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"dbfs:/databricks-datasets/README.md\"\n",
    "\n",
    "df = spark.read.text(filename)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7a55a34-7186-4b7d-9477-a6fd977b7cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://docs.databricks.com/data/databricks-file-system.html#spark-apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36cc01b7-dee7-42b9-af88-95e5a05364ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark computations are expressed as operations. These operations are then converted into low-level RDD-based bytecode as tasks, which are distributed to Spark’s executors for execution.\n",
    "\n",
    "(Page 23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2bcc86ec-92cf-43eb-a1c5-f2182c3b449a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_spark",
   "notebookOrigID": 3955027054878432,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

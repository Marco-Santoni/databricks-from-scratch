{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "917a7930-8e7b-4624-9b29-88359dfd0f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's create a first notebook and attach it to a cluster. We can choose a `Python` module and make us of PySpark. Plea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f681e172-ae95-4457-b663-ca9fe3f31d7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a852dd38-b31a-4038-8d44-d89ca3240cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where are we actually sitting? What files are available to us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5bb1cfa1-5717-484c-a25c-a83acbd81a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "535b6f5f-868d-47ec-b0cb-daa33055b73d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81ecd4bc-f90e-48cf-8c44-fa8928b0ca8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls databricks-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33baa889-f3df-4db9-b30a-8f241424c6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "[DBFS](https://docs.databricks.com/data/databricks-file-system.html) is an abstraction on top of scalable object storage and offers the following benefits:\n",
    "\n",
    "- Allows you to mount storage objects so that you can seamlessly access data without requiring credentials.\n",
    "- Allows you to interact with object storage using directory and file semantics instead of storage URLs.\n",
    "- Persists files to object storage, so you won’t lose data after you terminate a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57cb8734-2e47-4439-a165-403cf29b2a99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where can I play around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe7dcdd9-403d-4605-a647-c05e7000b055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls FileStore/shared_uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c818ff6-ef91-4bca-86e1-c703dabdc345",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read\n",
    "\n",
    "We'll now read some files as strings and as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b73a4a8d-bfd4-4e25-a4f5-c13c341000f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"dbfs:/databricks-datasets/README.md\"\n",
    "\n",
    "df = spark.read.text(filename)\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2bcc86ec-92cf-43eb-a1c5-f2182c3b449a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eace2239-fb76-4a2b-a1f2-e4f2218da31b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataframes\n",
    "\n",
    "Then, what is this `DataFrame` class?\n",
    "\n",
    "A [DataFrame](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html) is similar to a relational table in SQL or to a Pandas dataframe.\n",
    "\n",
    "Once created, it can be manipulated using the various domain-specific-language (DSL) functions. We'll find out more about them over this training.\n",
    "\n",
    "Another data structure that can be used to process data in spark is an [RDD](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.html#pyspark.RDD), a Resilient Distributed Dataset, the basic abstraction in Spark. Throughout this course, we will focus more on the usage of `DataFrame`s. Since Spark 2.x, RDD are now consigned to low-level APIs and are not a recommended tool for common use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7be7ba71-b73b-4f78-aef2-8a2a5dd0c217",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What's happening behind the scene?\n",
    "\n",
    "To understand what’s happening under the hood with our sample code, you’ll need to be familiar with some of the key concepts of a Spark application and how the code is transformed and executed as tasks across the Spark executors. We’ll begin by defining some important terms:\n",
    "\n",
    "- **Application**. A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.\n",
    "- **SparkSession**. An object that provides a point of entry to interact with underlying Spark functionality and allows programming Spark with its APIs. In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession object yourself.\n",
    "- **Job**. A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).\n",
    "- **Stage**. Each job gets divided into smaller sets of tasks called stages that depend on each other.\n",
    "- **Task** A single unit of work or execution that will be sent to a Spark executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6e5169e4-a4aa-491b-8d1f-ff964cf7b44a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now move back to ours slides!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_spark",
   "notebookOrigID": 3955027054878432,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

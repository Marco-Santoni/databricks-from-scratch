{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "917a7930-8e7b-4624-9b29-88359dfd0f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's create a first notebook and attach it to a cluster. We can choose a `Python` module and make us of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f681e172-ae95-4457-b663-ca9fe3f31d7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a852dd38-b31a-4038-8d44-d89ca3240cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where are we actually sitting? What files are available to us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5bb1cfa1-5717-484c-a25c-a83acbd81a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "535b6f5f-868d-47ec-b0cb-daa33055b73d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81ecd4bc-f90e-48cf-8c44-fa8928b0ca8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls databricks-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33baa889-f3df-4db9-b30a-8f241424c6e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "[DBFS](https://docs.databricks.com/data/databricks-file-system.html) is an abstraction on top of scalable object storage and offers the following benefits:\n",
    "\n",
    "- Allows you to mount storage objects so that you can seamlessly access data without requiring credentials.\n",
    "- Allows you to interact with object storage using directory and file semantics instead of storage URLs.\n",
    "- Persists files to object storage, so you won’t lose data after you terminate a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57cb8734-2e47-4439-a165-403cf29b2a99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Where can I play around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe7dcdd9-403d-4605-a647-c05e7000b055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls FileStore/shared_uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c818ff6-ef91-4bca-86e1-c703dabdc345",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read\n",
    "\n",
    "We'll now read some files as strings and as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b73a4a8d-bfd4-4e25-a4f5-c13c341000f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"dbfs:/databricks-datasets/README.md\"\n",
    "\n",
    "df = spark.read.text(filename)\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2bcc86ec-92cf-43eb-a1c5-f2182c3b449a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36cc01b7-dee7-42b9-af88-95e5a05364ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataframes and RDDs\n",
    "\n",
    "Spark computations are expressed as operations. These operations are then converted into low-level RDD-based bytecode as tasks, which are distributed to Spark’s executors for execution.\n",
    "\n",
    "Also note that we used the high-level Structured APIs to read a text file into a Spark DataFrame rather than an RDD. Throughout the book, we will focus more on these Structured APIs; since Spark 2.x, RDDs are now consigned to low-level APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7be7ba71-b73b-4f78-aef2-8a2a5dd0c217",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To understand what’s happening under the hood with our sample code, you’ll need to be familiar with some of the key concepts of a Spark application and how the code is transformed and executed as tasks across the Spark executors. We’ll begin by defining some important terms:\n",
    "\n",
    "- **Application**. A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.\n",
    "- **SparkSession**. An object that provides a point of entry to interact with underlying Spark functionality and allows programming Spark with its APIs. In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession object yourself.\n",
    "- **Job**. A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).\n",
    "- **Stage**. Each job gets divided into smaller sets of tasks called stages that depend on each other.\n",
    "- **Task** A single unit of work or execution that will be sent to a Spark executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6e5169e4-a4aa-491b-8d1f-ff964cf7b44a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now move back to ours slides!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "723e2215-6a45-4a14-8690-ce237bf8f72f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_spark",
   "notebookOrigID": 3955027054878432,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{"cells":[{"cell_type":"markdown","source":["[_Ref: Learning Spark - Chapter 3, Damji et al_]\n\nThere are three key Spark interfaces that you should know about. [Ref](https://databricks.com/spark/getting-started-with-apache-spark/quick-start#spark-interfaces)\n\n- RDD (*original data structure for Apache Spark*)\n- üíó DataFrame (*most common today*)\n- Datasets (*Java and Scala only*)\n\nThe RDD is the most basic abstraction in Spark, a simple programming API model upon which all higher-level functionality is constructed. However, RDD APIs are not expressive, and you don't want to use them in most of the scenarios.\n\nSpark 2.x introduced a few key schemes for structuring Spark. One is to express com‚Äê putations by using common patterns found in data analysis. These patterns are expressed as high-level operations such as filtering, selecting, counting, aggregating, averaging, and grouping. This provides added clarity and simplicity.\n\n### Is it efficient? - Opaqueness of RDD\n\nThe computation of an RDD is opaque to Spark. That is, Spark does not know what you are doing in the compute function. Whether you are performing a join, filter, select, or aggregation, Spark only sees it as a lambda expression. Another problem is that the data type is also opaque for Python RDDs; Spark only knows that it‚Äôs a generic object in Python.\n\nThis opacity clearly hampers Spark‚Äôs ability to rearrange your computation into an efficient query plan.\n\nWhat about **DataFrame** APIs? Their operators let you tell Spark what you wish to compute with your data, and as a result, it can construct an efficient query plan for execution. Structure yields a number of benefits, including better performance and space efficiency across Spark components."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13055c75-e8a7-4d45-b220-3154eaad13d5"}}},{"cell_type":"markdown","source":["## DataFrame APIs\n\nA `DataFrame` looks like a table to us, and they are actually distributed in-memory tables (and remind us of Pandas' DataFrames).\n\nEach column is assigned a [data type](https://spark.apache.org/docs/latest/sql-ref-datatypes.html). They can be summarized in two main categories\n\n- basic data types\n  - numeric (`IntegerType`, `FloatType`, etc)\n  - strings (`StringType`, etc)\n  - boolean\n- structured and complex types\n  - date and time (`TimestampType`, `DateType`, `DayTimeIntervalType`, etc)\n  - structures (`ArrayType`, `MapType`, `StructField`)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42a6f1c5-ec76-4560-a950-61640c5928ef"}}},{"cell_type":"code","source":["# data types can be inferred automatically by Spark\ndf = spark.read.csv('dbfs:/databricks-datasets/learning-spark-v2/mnm_dataset.csv', header=True)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"213ce4fd-1e25-40e1-8657-6d8cf89ef4d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+-----+\n|State| Color|Count|\n+-----+------+-----+\n|   TX|   Red|   20|\n|   NV|  Blue|   66|\n|   CO|  Blue|   79|\n|   OR|  Blue|   71|\n|   WA|Yellow|   93|\n|   WY|  Blue|   16|\n|   CA|Yellow|   53|\n|   WA| Green|   60|\n|   OR| Green|   71|\n|   TX| Green|   68|\n|   NV| Green|   59|\n|   AZ| Brown|   95|\n|   WA|Yellow|   20|\n|   AZ|  Blue|   75|\n|   OR| Brown|   72|\n|   NV|   Red|   98|\n|   WY|Orange|   45|\n|   CO|  Blue|   52|\n|   TX| Brown|   94|\n|   CO|   Red|   82|\n+-----+------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+-----+\n|State| Color|Count|\n+-----+------+-----+\n|   TX|   Red|   20|\n|   NV|  Blue|   66|\n|   CO|  Blue|   79|\n|   OR|  Blue|   71|\n|   WA|Yellow|   93|\n|   WY|  Blue|   16|\n|   CA|Yellow|   53|\n|   WA| Green|   60|\n|   OR| Green|   71|\n|   TX| Green|   68|\n|   NV| Green|   59|\n|   AZ| Brown|   95|\n|   WA|Yellow|   20|\n|   AZ|  Blue|   75|\n|   OR| Brown|   72|\n|   NV|   Red|   98|\n|   WY|Orange|   45|\n|   CO|  Blue|   52|\n|   TX| Brown|   94|\n|   CO|   Red|   82|\n+-----+------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["However, it is also possible to define the schema and the data type **before** reading the data (and this is actually recommended).\n\n‚ùìüôã‚Äç‚ôÄÔ∏èüôã‚Äç‚ôÇÔ∏è\nWhy is it recommended to define the schema before reading a dataset?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c07d31ff-932f-4190-9b1f-3cc1520e1ff9"}}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95c46166-471a-4a31-8f2b-9d0743f7fcec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Three main advantages\n\n1. performance. When Spark has to infer the schema, it reads part of the dataset just to make this inference. It can be avoided if you tell Spark what data types you expect from each column.\n2. precision. Spark might choose the wrong data types when inferring.\n3. errors. There can be errors in some datasets (wrong formatting, etc). These errors can be spotted if the data does not match the data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcef990a-a2d2-417f-9bfe-ec04007a9634"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"960d54f9-e0fb-470c-9a4c-87d1eba1ae33"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3_structured_api","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3918717434270984}},"nbformat":4,"nbformat_minor":0}

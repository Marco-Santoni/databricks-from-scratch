{"cells":[{"cell_type":"markdown","source":["[_Ref: Learning Spark - Chapter 3, Damji et al_]\n\nThere are three key Spark interfaces that you should know about. [Ref](https://databricks.com/spark/getting-started-with-apache-spark/quick-start#spark-interfaces)\n\n- RDD (*original data structure for Apache Spark*)\n- üíó DataFrame (*most common today*)\n- Datasets (*Java and Scala only*)\n\nThe RDD is the most basic abstraction in Spark, a simple programming API model upon which all higher-level functionality is constructed. However, RDD APIs are not expressive, and you don't want to use them in most of the scenarios.\n\nSpark 2.x introduced a few key schemes for structuring Spark. One is to express com‚Äê putations by using common patterns found in data analysis. These patterns are expressed as high-level operations such as filtering, selecting, counting, aggregating, averaging, and grouping. This provides added clarity and simplicity.\n\n### Is it efficient? - Opaqueness of RDD\n\nThe computation of an RDD is opaque to Spark. That is, Spark does not know what you are doing in the compute function. Whether you are performing a join, filter, select, or aggregation, Spark only sees it as a lambda expression. Another problem is that the data type is also opaque for Python RDDs; Spark only knows that it‚Äôs a generic object in Python.\n\nThis opacity clearly hampers Spark‚Äôs ability to rearrange your computation into an efficient query plan.\n\nWhat about **DataFrame** APIs? Their operators let you tell Spark what you wish to compute with your data, and as a result, it can construct an efficient query plan for execution. Structure yields a number of benefits, including better performance and space efficiency across Spark components."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13055c75-e8a7-4d45-b220-3154eaad13d5"}}},{"cell_type":"markdown","source":["## DataFrame APIs\n\nA `DataFrame` looks like a table to us, and they are actually distributed in-memory tables (and remind us of Pandas' DataFrames).\n\nEach column is assigned a [data type](https://spark.apache.org/docs/latest/sql-ref-datatypes.html). They can be summarized in two main categories\n\n- basic data types\n  - numeric (`IntegerType`, `FloatType`, etc)\n  - strings (`StringType`, etc)\n  - boolean\n- structured and complex types\n  - date and time (`TimestampType`, `DateType`, `DayTimeIntervalType`, etc)\n  - structures (`ArrayType`, `MapType`, `StructField`)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42a6f1c5-ec76-4560-a950-61640c5928ef"}}},{"cell_type":"code","source":["# data types can be inferred automatically by Spark\ndf = spark.read.csv('dbfs:/databricks-datasets/learning-spark-v2/mnm_dataset.csv', header=True)\ndf.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"213ce4fd-1e25-40e1-8657-6d8cf89ef4d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-----+-----+\n|State|Color|Count|\n+-----+-----+-----+\n|   TX|  Red|   20|\n|   NV| Blue|   66|\n|   CO| Blue|   79|\n+-----+-----+-----+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-----+-----+\n|State|Color|Count|\n+-----+-----+-----+\n|   TX|  Red|   20|\n|   NV| Blue|   66|\n|   CO| Blue|   79|\n+-----+-----+-----+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["However, it is also possible to define the schema and the data type **before** reading the data (and this is actually recommended).\n\n‚ùìüôã‚Äç‚ôÄÔ∏èüôã‚Äç‚ôÇÔ∏è\nWhy is it recommended to define the schema before reading a dataset?\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c07d31ff-932f-4190-9b1f-3cc1520e1ff9"}}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95c46166-471a-4a31-8f2b-9d0743f7fcec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Three main advantages\n\n1. performance. When Spark has to infer the schema, it reads part of the dataset just to make this inference. It can be avoided if you tell Spark what data types you expect from each column.\n2. precision. Spark might choose the wrong data types when inferring.\n3. errors. There can be errors in some datasets (wrong formatting, etc). These errors can be spotted if the data does not match the data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcef990a-a2d2-417f-9bfe-ec04007a9634"}}},{"cell_type":"code","source":["# Let's define a schema BEFORE reading the data\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nschema = StructType([\n    StructField(\"State\", StringType(), False),\n    StructField(\"Color\", StringType(), False),\n    StructField(\"Count\", IntegerType(), False)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"960d54f9-e0fb-470c-9a4c-87d1eba1ae33"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv(\n    'dbfs:/databricks-datasets/learning-spark-v2/mnm_dataset.csv',\n    schema=schema,\n    header=True\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf4310ea-ffaf-48ff-bc72-a9d287552b81"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4732bc0b-8bb0-48a3-943d-a959ef880987"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: integer (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- State: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- Count: integer (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"265f27ae-b7cf-4138-b79d-76cbaa6dbe18"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-----+-----+\n|State|Color|Count|\n+-----+-----+-----+\n|   TX|  Red|   20|\n|   NV| Blue|   66|\n|   CO| Blue|   79|\n+-----+-----+-----+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-----+-----+\n|State|Color|Count|\n+-----+-----+-----+\n|   TX|  Red|   20|\n|   NV| Blue|   66|\n|   CO| Blue|   79|\n+-----+-----+-----+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Writing a schema from scratch is tedious. Can I speed it up? Yes, we can print the schema definition of an existing schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f62c20ad-f787-435d-9914-ef05929022be"}}},{"cell_type":"code","source":["df.schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cab221-1547-4acc-b441-c99e2c4e39d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[7]: StructType(List(StructField(State,StringType,true),StructField(Color,StringType,true),StructField(Count,IntegerType,true)))","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[7]: StructType(List(StructField(State,StringType,true),StructField(Color,StringType,true),StructField(Count,IntegerType,true)))"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Columns and expressions\n\nWhat are actually the columns of a `DataFrame`? They are objects, `Column` objects. They can be accessed in different ways including the `col` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43528709-9fcf-49dc-bc9b-1a9699719292"}}},{"cell_type":"code","source":["df.Count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"293968bc-ee87-48f2-b21c-6c9357d28fd6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: Column<'Count'>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: Column<'Count'>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af80fbfc-71e8-4fb1-882c-38c18013f7a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["col('Count')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f82bce46-5930-40bb-9d54-b55da56f61fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: Column<'Count'>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: Column<'Count'>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Let's see what we can do with columns and how we can transform them."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c74c6e2-1a9e-4d6c-a55e-aab0ae19f34e"}}},{"cell_type":"code","source":["# select columns using col\ndf.select(col('Count')).show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7093aafe-5a09-42f1-a77e-a869a53124bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+\n|Count|\n+-----+\n|   20|\n|   66|\n|   79|\n+-----+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+\n|Count|\n+-----+\n|   20|\n|   66|\n|   79|\n+-----+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# select columns using DataFrame.NameOfColumn\ndf.select(df.Count).show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c182847a-be98-4b0c-8679-4c6294313ee1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+\n|Count|\n+-----+\n|   20|\n|   66|\n|   79|\n+-----+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+\n|Count|\n+-----+\n|   20|\n|   66|\n|   79|\n+-----+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# We can transform a column via expr or using direct python operators\ndf.select(df.Count * 10).show(3)\n\nfrom pyspark.sql.functions import expr\ndf.select(expr('Count * 10')).show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6666b0e-10a4-479e-883b-63d4208e2aa4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+\n|(Count * 10)|\n+------------+\n|         200|\n|         660|\n|         790|\n+------------+\nonly showing top 3 rows\n\n+------------+\n|(Count * 10)|\n+------------+\n|         200|\n|         660|\n|         790|\n+------------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+\n|(Count * 10)|\n+------------+\n|         200|\n|         660|\n|         790|\n+------------+\nonly showing top 3 rows\n\n+------------+\n|(Count * 10)|\n+------------+\n|         200|\n|         660|\n|         790|\n+------------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# With withColumn, we can define a new column that is computed based on other columns' values\ndf.withColumn('halfCount', df.Count / 2).show(3)\ndf.withColumn('halfCount', col('Count') / 2).show(3)\ndf.withColumn('halfCount', expr('Count / 2')).show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1989e66d-6144-44d8-baef-7529fbe54f40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+-----+-----+---------+\n|State|Color|Count|halfCount|\n+-----+-----+-----+---------+\n|   TX|  Red|   20|     10.0|\n|   NV| Blue|   66|     33.0|\n|   CO| Blue|   79|     39.5|\n+-----+-----+-----+---------+\nonly showing top 3 rows\n\n+-----+-----+-----+---------+\n|State|Color|Count|halfCount|\n+-----+-----+-----+---------+\n|   TX|  Red|   20|     10.0|\n|   NV| Blue|   66|     33.0|\n|   CO| Blue|   79|     39.5|\n+-----+-----+-----+---------+\nonly showing top 3 rows\n\n+-----+-----+-----+---------+\n|State|Color|Count|halfCount|\n+-----+-----+-----+---------+\n|   TX|  Red|   20|     10.0|\n|   NV| Blue|   66|     33.0|\n|   CO| Blue|   79|     39.5|\n+-----+-----+-----+---------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+-----+-----+---------+\n|State|Color|Count|halfCount|\n+-----+-----+-----+---------+\n|   TX|  Red|   20|     10.0|\n|   NV| Blue|   66|     33.0|\n|   CO| Blue|   79|     39.5|\n+-----+-----+-----+---------+\nonly showing top 3 rows\n\n+-----+-----+-----+---------+\n|State|Color|Count|halfCount|\n+-----+-----+-----+---------+\n|   TX|  Red|   20|     10.0|\n|   NV| Blue|   66|     33.0|\n|   CO| Blue|   79|     39.5|\n+-----+-----+-----+---------+\nonly showing top 3 rows\n\n+-----+-----+-----+---------+\n|State|Color|Count|halfCount|\n+-----+-----+-----+---------+\n|   TX|  Red|   20|     10.0|\n|   NV| Blue|   66|     33.0|\n|   CO| Blue|   79|     39.5|\n+-----+-----+-----+---------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# we can concatenate columns too\nfrom pyspark.sql.functions import concat\ndf.withColumn('StateColor', concat(df.State, df.Color))\\\n  .select('StateColor').show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45c1d883-24fe-44de-bb20-199bf8da1327"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+\n|StateColor|\n+----------+\n|     TXRed|\n|    NVBlue|\n|    COBlue|\n+----------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+\n|StateColor|\n+----------+\n|     TXRed|\n|    NVBlue|\n|    COBlue|\n+----------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# what if I'd like to split them with a '-'?\ndf.withColumn('StateColor', concat(df.State, ' - ', df.Color))\\\n  .select('StateColor').show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b97296ac-e12c-464a-a0ad-9089cd8eb4fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1331452190994784>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# what if I'd like to split them with a '-'?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'StateColor'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mState\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m' - '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mColor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'StateColor'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   2652\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2653\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2654\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2655\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2656\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column '` - `' does not exist. Did you mean one of the following? [Color, Count, State];\n'Project [State#94, Color#95, Count#96, concat(State#94, ' - , Color#95) AS StateColor#235]\n+- Relation [State#94,Color#95,Count#96] csv\n","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Column '` - `' does not exist. Did you mean one of the following? [Color, Count, State];\n'Project [State#94, Color#95, Count#96, concat(State#94, ' - , Color#95) AS StateColor#235]\n+- Relation [State#94,Color#95,Count#96] csv\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1331452190994784>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# what if I'd like to split them with a '-'?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'StateColor'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mState\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m' - '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mColor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m   \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'StateColor'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mwithColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   2652\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2653\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col should be Column\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2654\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolName\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2655\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2656\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexisting\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column '` - `' does not exist. Did you mean one of the following? [Color, Count, State];\n'Project [State#94, Color#95, Count#96, concat(State#94, ' - , Color#95) AS StateColor#235]\n+- Relation [State#94,Color#95,Count#96] csv\n"]}}],"execution_count":0},{"cell_type":"code","source":["# we cannot pass a string because concat is expecting a Column input\n# we can use the lit function to add a constant or a literal to a dataframe\nfrom pyspark.sql.functions import lit\ndf.withColumn('StateColor', concat(df.State, lit(' - '), df.Color))\\\n  .select('StateColor').show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a17e8013-84ed-4690-b309-e98d569a5e20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+\n|StateColor|\n+----------+\n|  TX - Red|\n| NV - Blue|\n| CO - Blue|\n+----------+\nonly showing top 3 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+\n|StateColor|\n+----------+\n|  TX - Red|\n| NV - Blue|\n| CO - Blue|\n+----------+\nonly showing top 3 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# We can sort a dataframe according to the value of a column\ndf.sort(df.Count.desc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f34afe3c-ba19-4663-ace0-1d4f3166aa83"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------+-----+\n|State| Color|Count|\n+-----+------+-----+\n|   CA| Brown|  100|\n|   WY| Green|  100|\n|   NV|   Red|  100|\n|   TX|   Red|  100|\n|   CA|   Red|  100|\n|   UT|   Red|  100|\n|   WY|  Blue|  100|\n|   UT|Yellow|  100|\n|   AZ|Orange|  100|\n|   CO|   Red|  100|\n|   TX|Yellow|  100|\n|   CO| Green|  100|\n|   NV|   Red|  100|\n|   CA| Brown|  100|\n|   UT|Yellow|  100|\n|   CA|   Red|  100|\n|   NM| Green|  100|\n|   NV|   Red|  100|\n|   WA| Brown|  100|\n|   NM|Orange|  100|\n+-----+------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------+-----+\n|State| Color|Count|\n+-----+------+-----+\n|   CA| Brown|  100|\n|   WY| Green|  100|\n|   NV|   Red|  100|\n|   TX|   Red|  100|\n|   CA|   Red|  100|\n|   UT|   Red|  100|\n|   WY|  Blue|  100|\n|   UT|Yellow|  100|\n|   AZ|Orange|  100|\n|   CO|   Red|  100|\n|   TX|Yellow|  100|\n|   CO| Green|  100|\n|   NV|   Red|  100|\n|   CA| Brown|  100|\n|   UT|Yellow|  100|\n|   CA|   Red|  100|\n|   NM| Green|  100|\n|   NV|   Red|  100|\n|   WA| Brown|  100|\n|   NM|Orange|  100|\n+-----+------+-----+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Exercise\n\nConsider the dataset `/databricks-datasets/flights/departuredelays.csv` about flights and delays.\n\n1. Import the csv in a `DataFrame`. Would you define the schema before?\n2. The column `delay` expresses the delay in minutes. Can you compute a new column `delayInHours` where the amount of `delay` is converted to hours?\n3. What is the flight with largest delay ever?\n4. [Bonus] What is the most popular route? Note that a route is the combination of an `origin` and a `destination`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c744df51-6277-4dfe-8bf2-1752b1a0af1a"}}},{"cell_type":"markdown","source":["## Rows\n\nA row in Spark is a generic [Row](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.Row.html) object, containing one or more columns.\nThe fields in it can be accessed:\n\n- like attributes (`row.key`)\n- like dictionary values (`row[key]`)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5423f695-d238-497d-822b-2244ef71cd97"}}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nRow('Alice', 11)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1abcfe3f-6181-4772-a201-778a87103b3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: <Row('Alice', 11)>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: <Row('Alice', 11)>"]}}],"execution_count":0},{"cell_type":"code","source":["# Or using named arguments\nRow(name=\"Alice\", age=11)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9349eca8-d557-453e-b295-c8eeb5c45426"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[20]: Row(name='Alice', age=11)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[20]: Row(name='Alice', age=11)"]}}],"execution_count":0},{"cell_type":"code","source":["alice_row = Row(name=\"Alice\", age=11)\n# we can access values via index or via key\nprint(alice_row[0])\nprint(alice_row['name'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24b749d6-19cd-4a0d-8a51-305560bdbc5c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Alice\nAlice\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Alice\nAlice\n"]}}],"execution_count":0},{"cell_type":"code","source":["# you can create quickly dataframes for prototyping\nrows = [\n    alice_row,\n    Row(name='Bob', age=15)\n]\nspark.createDataFrame(rows).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e13e8c56-b291-46da-94e0-7682847d5412"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+---+\n| name|age|\n+-----+---+\n|Alice| 11|\n|  Bob| 15|\n+-----+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+---+\n| name|age|\n+-----+---+\n|Alice| 11|\n|  Bob| 15|\n+-----+---+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## User Defined Functions\n\nYou can define your own custom functions to transform `DataFrame`s. They are called _User Defined Functions_ (UDF)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db1dfa88-6c45-4a73-b82c-5de8c360e779"}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\n@udf\ndef to_upper(some_string):\n    if some_string is not None:\n        return some_string.upper()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65c9d1d0-a56d-41f8-bf30-bfa28a50bd6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select(\"Color\", to_upper(col(\"Color\"))).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ccd7062-3b12-4e38-ad97-ace398b885a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+---------------+\n| Color|to_upper(Color)|\n+------+---------------+\n|   Red|            RED|\n|  Blue|           BLUE|\n|  Blue|           BLUE|\n|  Blue|           BLUE|\n|Yellow|         YELLOW|\n+------+---------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+---------------+\n| Color|to_upper(Color)|\n+------+---------------+\n|   Red|            RED|\n|  Blue|           BLUE|\n|  Blue|           BLUE|\n|  Blue|           BLUE|\n|Yellow|         YELLOW|\n+------+---------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3_structured_api","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3918717434270984}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","source":["## Exercise - binary classification\n\n### Audult dataset\n\n`databricks-datasets/adult/adult.data`\n\nThis data derives from census data and consists of information about 48842 individuals and their annual income.\nYou can use this information to predict if an individual earns **<=50K or >50k** a year.\nThe dataset consists of both numeric and categorical variables.\n\nAttribute Information:\n\n- age: continuous\n- workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n- fnlwgt: continuous\n- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n- education-num: continuous\n- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n- sex: Female, Male\n- capital-gain: continuous\n- capital-loss: continuous\n- hours-per-week: continuous\n- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n\nTarget/Label: - <=50K, >50K\n\n### Part 1\n\nSplit the dataset between _train_ and _test_ (`.80` and `.20` respectively using a random seed of `42`). On the _train_ set, fit a `LogisticRegression` on the following features\n\n- `age`\n- `education`\n- `sex`\n- `is_us_native`: this is a binary feature you should build based on `native-country`\n- `hours-per-week`\n\nNow, run the predictions on the _test_ set and compute the following metrics using `BinaryClassificationEvaluator`:\n\n- `areaUnderPR`\n- `areaUnderROC`\n\nWhat are the `coefficients` and the `intercept` of your fitted model?\n\n### Part 2\n\nCompare the performance of the previous model with a `RandomForestClassifier`. Which one is giving the best `areaUnderROC` score?\n\nTake the best model and run predictions on the entire dataset. On these predictions, compute the confusion matrix. Example\n\n| | <=50k (predicted) | > 50k (predicted)|\n|--|--|--|\n| <=50k (true) | 12 | 34|\n| >50k (true) | 56 | 78|\n\n### Part 3 - Bonus\n\nInstead of computing the feature `is_us_native` in the original dataset, can you define a custom `Transformer` that computes this new feature in a step of the pipeline?\n\nHere, an [example](https://towardsdatascience.com/pyspark-wrap-your-feature-engineering-in-a-pipeline-ee63bdb913) of how it could be done."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3d76898-bd97-4d7f-87df-a83bc27ece30"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"9_more_ml_exercises","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3318196723325470}},"nbformat":4,"nbformat_minor":0}
